{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser('http://localhost:9001')\n",
    "from nltk.tree import Tree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from opencc import OpenCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下來串接\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, argparse, time, random\n",
    "from model import BiLSTM_CRF\n",
    "from utils import str2bool, get_logger, get_entity\n",
    "from data import read_corpus, read_dictionary, tag2label, random_embedding, vocab_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新聞網站爬蟲 + 寫入檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    #模擬訪問頁面的函數\n",
    "    try:\n",
    "        user_agent = 'Mozilla/5.0'\n",
    "        resp = requests.get(url, headers={'User-Agent': user_agent}, timeout = 30) #回傳為一個request.Response的物件\n",
    "        resp.endcoding = 'utf8'\n",
    "        #response = resp.content.decode()\n",
    "        #c = resp.status_code\n",
    "        return resp.text\n",
    "    except:\n",
    "        return 'ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apple_news(keyword):\n",
    "    #函數用來爬取特定關鍵字下的新聞內容，函數內可以再設定要爬取的頁數（或之後用時間來選），但蘋果日報需要先登入才能用（這部分還沒寫自動化）\n",
    "    page_num = 10\n",
    "    url_list = []\n",
    "    for i in range(page_num):\n",
    "        html = get_html('https://tw.appledaily.com/search/ajaxresult?querystrS='+ keyword +'&page='+str(i+1))\n",
    "        json_parse = json.loads(html)\n",
    "        for j in json_parse:\n",
    "            url_list.append(j['sharing']['url'])\n",
    "        #print(json_parse)\n",
    "    #print(url_list)\n",
    "    #開始訪問每個新聞連結並取得內容，最後寫進檔案\n",
    "    w = open(keyword + \"_all_apple_news.txt\", 'w')\n",
    "    for i in url_list:\n",
    "        text = get_html(i)\n",
    "        html = etree.HTML(text)\n",
    "        news_content = html.xpath('//*[@id=\"article\"]/div[1]/div/main/article/div[2]/div[2]/article/div/p[1]/text()')\n",
    "        if news_content == []:\n",
    "            news_content = html.xpath('//*[@id=\"article\"]/div[1]/div/main/article/div[3]/article/div/p[1]/text()')#回傳list\n",
    "        #把list變成一個字串\n",
    "        news_content_str = str() \n",
    "        for i in news_content:\n",
    "            news_content_str += i\n",
    "        w.write(news_content_str)\n",
    "        w.write('\\n')\n",
    "    w.close()\n",
    "    \n",
    "    return \"Finish\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_china_times_news(keyword):\n",
    "    #函數用來爬取特定關鍵字下的新聞內容，函數內可以再設定要爬取的頁數（或之後用時間來選）\n",
    "    page_num = 10\n",
    "    url_list = []\n",
    "    #取得指定搜尋頁數裡的新聞url，並存成url_list\n",
    "    for i in range(page_num):\n",
    "        html = get_html('https://www.chinatimes.com/search/'+ keyword +'?page='+str(i+1)+'&chdtv')\n",
    "        tree = etree.HTML(html)\n",
    "        for j in range(13):\n",
    "            url = tree.xpath('/html/body/div[2]/div/div[2]/div/section/div/ul/li['+ str(j+1) +']/div/div/div/h3/a/@href')\n",
    "            url_list.append(\"\".join(url))\n",
    "    while '' in url_list: #廣告是空的，每一頁13個url中有3個是廣告，但抓下來都是空的，就去掉\n",
    "        url_list.remove('')\n",
    "    for i in range(len(url_list)):\n",
    "        url_list[i] += '?chdtv'\n",
    "    print(url_list)\n",
    "     \n",
    "    #開始訪問每個新聞連結並取得內容，最後寫進檔案\n",
    "    w = open(keyword + \"_all_china_times_news.txt\", 'w')\n",
    "    for i in url_list:\n",
    "        text = get_html(i)\n",
    "        html = etree.HTML(text)\n",
    "        news_header = html.xpath('//*[@id=\"page-top\"]/div/div[2]/div/div/article/div/header/h1/text()')\n",
    "        news_content = html.xpath('//*[@id=\"page-top\"]/div/div[2]/div/div/article/div/div[1]/div[2]/div[2]/div[2]//text()')\n",
    "        news_content_str = str()\n",
    "        news_content_str += \"\".join(news_header)\n",
    "        news_content_str += \"\\n\"\n",
    "        for j in news_content:\n",
    "            if j[0] != \"\\n\":\n",
    "                news_content_str += j\n",
    "        w.write(news_content_str)\n",
    "        #print(news_content_str)\n",
    "        w.write('\\n')\n",
    "        #time.sleep(1)應該不是一次太多request的問題\n",
    "    \n",
    "    print(news_content_str)\n",
    "    w.close()\n",
    "    return 'yeah'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(keyword,idx=2):\n",
    "    keyword_link = []\n",
    "    url = \"https://www.ettoday.net/news_search/doSearch.php?keywords=\"\n",
    "    for i in keyword:\n",
    "        keyword_link.append(url+i+'&idx='+str(idx))\n",
    "    return keyword_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(string):\n",
    "    page = str()\n",
    "    flag=1\n",
    "    for i in range(len(string)):\n",
    "        if string[i]=='頁':\n",
    "            flag=1\n",
    "        if flag==0:\n",
    "            page+=(string[i])\n",
    "        if string[i]=='共':\n",
    "            flag = 0\n",
    "    return int(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_link(url):\n",
    "    try:\n",
    "        news_link = []\n",
    "        total_page = 10000\n",
    "        page_num = 1\n",
    "        while page_num <= total_page:\n",
    "            html = get_html(url)\n",
    "            soup = BeautifulSoup(html,\"html.parser\")\n",
    "            sel = soup.select(\"div.result_archive div.box_1 a\")\n",
    "            pages = soup.select(\"div.menu_page p\")\n",
    "            total_page = get_page(pages[0].text)\n",
    "            if not total_page:\n",
    "                return '您所輸入的查詢條件查無資料，請重新設定查詢條件！'\n",
    "            url = url+\"&page=\"+str(page_num)\n",
    "            page_num+=1\n",
    "            for i in sel:\n",
    "                news_link.append(i[\"href\"])\n",
    "        return news_link\n",
    "    except:\n",
    "        return '您所輸入的查詢條件查無資料，請重新設定查詢條件！'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_articles(url):\n",
    "    html = get_html(url)\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    sel = soup.select(\"div.story p\")\n",
    "    news_articles = str()\n",
    "    for i in sel:\n",
    "        if i.text!='' and ('strong' not in str(i)):\n",
    "            news_articles+=i.text\n",
    "    return news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_news(news_link):\n",
    "    news = []\n",
    "    if type(news_link)==str:\n",
    "        return '您所輸入的查詢條件查無資料，請重新設定查詢條件！'\n",
    "    for i in news_link:\n",
    "        news.append(get_news_articles(i))\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_news_dict(keyword):\n",
    "    keyword_link = get_link(keyword)\n",
    "    news_dict = {}\n",
    "    for i,link in enumerate(keyword_link):\n",
    "        news_link = get_news_link(link)\n",
    "        news = get_all_news(news_link)\n",
    "        news_dict[keyword[i]] = news\n",
    "    return news_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自檔案讀取文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text():\n",
    "    #函數讀取爬蟲下來的新聞\n",
    "    r = open('test_news.txt', 'r')\n",
    "    text = ''\n",
    "    for i in r:\n",
    "        text += i\n",
    "    sentence = ''\n",
    "    article = []\n",
    "    for i in text:   \n",
    "        if (i != '。') and (i != '\\n'):\n",
    "            sentence += i\n",
    "        else:\n",
    "            if sentence != '':\n",
    "                article.append(sentence)\n",
    "            sentence = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用NER自文章中取出名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Session configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # default: 0\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2  # need ~700MB GPU memory\n",
    "\n",
    "parser = argparse.ArgumentParser(description='BiLSTM-CRF for Chinese NER task')\n",
    "#parser.add_argument('--update_train_data', type=str, default='data_path', help='update training data set')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='data_path', help='train data source')\n",
    "parser.add_argument('--test_data', type=str, default='data_path', help='test data source')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='#sample of each minibatch')\n",
    "parser.add_argument('--epoch', type=int, default=40, help='#epoch of training')#default=40\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=300, help='#dim of hidden state')\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help='Adam/Adadelta/Adagrad/RMSProp/Momentum/SGD')\n",
    "parser.add_argument('--CRF', type=str2bool, default=True, help='use CRF at the top layer. if False, use Softmax')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--clip', type=float, default=5.0, help='gradient clipping')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='dropout keep_prob')\n",
    "parser.add_argument('--update_embedding', type=str2bool, default=True, help='update embedding during training')\n",
    "parser.add_argument('--pretrain_embedding', type=str, default='random', help='use pretrained char embedding or init it randomly')\n",
    "parser.add_argument('--embedding_dim', type=int, default=300, help='random init char embedding_dim')\n",
    "parser.add_argument('--shuffle', type=str2bool, default=True, help='shuffle training data before each epoch')\n",
    "parser.add_argument('--mode', type=str, default='demo', help='train/test/demo/update')\n",
    "parser.add_argument('--demo_model', type=str, default='1559725764', help='model for test and demo')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master\n"
     ]
    }
   ],
   "source": [
    "%cd zh-NER-TF-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Namespace(CRF=True, batch_size=64, clip=5.0, demo_model='1559725764', dropout=0.5, embedding_dim=300, epoch=40, hidden_dim=300, lr=0.001, mode='demo', optimizer='Adam', pretrain_embedding='random', shuffle=True, test_data='data_path', train_data='data_path', update_embedding=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_path_save/1559725764/results\n"
     ]
    }
   ],
   "source": [
    "## get char embeddings\n",
    "word2id = read_dictionary(os.path.join('.', args.train_data, 'word2id.pkl'))\n",
    "if args.pretrain_embedding == 'random':\n",
    "    embeddings = random_embedding(word2id, args.embedding_dim)\n",
    "else:\n",
    "    embedding_path = 'pretrain_embedding.npy'\n",
    "    embeddings = np.array(np.load(embedding_path), dtype='float32')\n",
    "\n",
    "\n",
    "train_path = os.path.join('.', args.train_data, 'train_data')\n",
    "test_path = os.path.join('.', args.test_data, 'test_data')\n",
    "train_data = read_corpus(train_path)\n",
    "test_data = read_corpus(test_path); test_size = len(test_data)\n",
    "\n",
    "paths = {}\n",
    "timestamp = str(int(time.time())) if args.mode == 'train' else args.demo_model\n",
    "output_path = os.path.join('.', args.train_data+\"_save\", timestamp)\n",
    "\n",
    "if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "    \n",
    "summary_path = os.path.join(output_path, \"summaries\")\n",
    "paths['summary_path'] = summary_path\n",
    "\n",
    "if not os.path.exists(summary_path): os.makedirs(summary_path)\n",
    "model_path = os.path.join(output_path, \"checkpoints/\")\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "ckpt_prefix = os.path.join(model_path, \"model\")\n",
    "paths['model_path'] = ckpt_prefix\n",
    "\n",
    "result_path = os.path.join(output_path, \"results\")\n",
    "paths['result_path'] = result_path\n",
    "\n",
    "if not os.path.exists(result_path): os.makedirs(result_path)\n",
    "log_path = os.path.join(result_path, \"log.txt\")\n",
    "paths['log_path'] = log_path\n",
    "\n",
    "get_logger(log_path).info(str(args))\n",
    "\n",
    "print(paths['result_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_path_save/1559725764/checkpoints/model-633400\n",
      "WARNING:tensorflow:From /Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master/model.py:60: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master/model.py:60: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master/model.py:64: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master/model.py:64: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master/model.py:71: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/rikeion/Desktop/Anti_Money_Laundry/zh-NER-TF-master/model.py:71: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "/Users/rikeion/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#USE MODEL\n",
    "ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "print(ckpt_file)\n",
    "paths['model_path'] = ckpt_file\n",
    "#build_graph用一次就好了~\n",
    "model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n",
    "model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_process_document(article):\n",
    "    #函數用來呼叫NER，讓NER能被多次使用\n",
    "    data = list()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, ckpt_file)\n",
    "        demo_sent = article.strip()\n",
    "        demo_data = [(demo_sent, ['O'] * len(demo_sent))]\n",
    "        tag = model.demo_one(sess, demo_data)\n",
    "        PER, LOC, ORG = get_entity(tag, demo_sent)\n",
    "        data.append({\n",
    "            'article' : article,\n",
    "            'per' : PER,\n",
    "            'loc' : LOC,\n",
    "            'org' : ORG\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_given_name():\n",
    "    #導入四百家姓\n",
    "    with open(\"../given_name_list\", \"r\") as r1:\n",
    "        given_name_list = []\n",
    "        for i in r1:\n",
    "            given_name_list.append(i.strip(\"\\n\"))\n",
    "    return given_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_name(per_list, given_name_list):\n",
    "    #函數用四百家姓及字數等規則來過濾NER判斷失敗的地方\n",
    "    new_per_list = []\n",
    "    for i in per_list:\n",
    "        if len(i) < 2 or len(i) > 4:\n",
    "            print(1)\n",
    "            continue\n",
    "        elif i[0] not in given_name_list:\n",
    "            print(2)\n",
    "            continue\n",
    "        elif not all('\\u4e00' <= char <= '\\u9fff' for char in i):\n",
    "            print(3)\n",
    "            continue\n",
    "        else: \n",
    "            new_per_list.append(i)\n",
    "            \n",
    "    return list(set(new_per_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradition2simple(sentence):\n",
    "    #函數將繁體中文轉成簡體中文\n",
    "    cc = OpenCC('t2s')\n",
    "    sentence = cc.convert(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple2tradition(sentence):\n",
    "    #函數將簡體中文轉成繁體中文\n",
    "    cc = OpenCC('s2t')\n",
    "    sentence = cc.convert(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_demo(sentence):\n",
    "    #組合上面函數，使用NER並加入判斷是否是名字，輸出名字的list\n",
    "    try:\n",
    "        #sentence = tradition2simple(sentence)\n",
    "        data = ner_process_document(sentence)\n",
    "        per_list = data[0]['per']\n",
    "        given_name_list = read_given_name()\n",
    "        new_per_list = scan_name(per_list, given_name_list)\n",
    "        print(per_list)\n",
    "        #new_per_list = #list(map(simple2tradition,new_per_list))\n",
    "    except:\n",
    "        return \"ERROR\"\n",
    "    return new_per_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找出同時包含名字及關鍵字之句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(article,keyword_list,name_list):\n",
    "    #函數用來找出同時包含名字及洗錢關鍵字的句子\n",
    "    sentence_contain_keyword = []\n",
    "    for i in range(len(article)):\n",
    "        for j in range(len(keyword_list)):\n",
    "            if article[i].find(keyword_list[j]) != -1:\n",
    "                sentence_contain_keyword.append(article[i])            \n",
    "    sentence_contain_keyword = set(sentence_contain_keyword)\n",
    "    \n",
    "    sentence_contain_name = []\n",
    "    for i in range(len(article)):\n",
    "        for j in range(len(name_list)):\n",
    "            if article[i].find(name_list[j]) != -1:\n",
    "                sentence_contain_name.append(article[i])            \n",
    "    sentence_contain_name = set(sentence_contain_name)\n",
    "    \n",
    "    sentence_wanted = set(sentence_contain_keyword) & set(sentence_contain_name)\n",
    "    sentence_wanted = list(sentence_wanted)\n",
    "    return sentence_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將名字及關鍵字寫入自訂辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_word(word,freq=10):\n",
    "    #將名字以及關鍵字寫入jieba辭典裡，以提高切詞正確率\n",
    "    with open('/Users/rikeion/Desktop/Anti_Money_Laundry/user_dict.txt','r') as f:\n",
    "        readword = f.read()\n",
    "        if word not in readword:\n",
    "            with open('/Users/rikeion/Desktop/Anti_Money_Laundry/user_dict.txt', 'a') as file:\n",
    "                file.write(\"\\n\"+word+\" \"+str(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匯入自訂辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/4s/mykg15b9135fc0jvn3h80fl00000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/4s/mykg15b9135fc0jvn3h80fl00000gn/T/jieba.cache\n",
      "Loading model cost 1.547 seconds.\n",
      "Loading model cost 1.547 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(\"/Users/rikeion/Desktop/Anti_Money_Laundry/user_dict.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用jieba切字並將句子製成樹狀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(sentence,keyword_list):\n",
    "    #先判斷句子中是否含有關鍵字，若無則回傳None;接著，將句子做成樹狀結構，以便解析句構\n",
    "    token = jieba.cut(sentence, cut_all = False)\n",
    "    flag = 0\n",
    "    token_list = list(token)\n",
    "    for i in keyword_list: \n",
    "        if simple2tradition(i) in token_list:\n",
    "            flag = 1\n",
    "    if flag!=1:\n",
    "        return None\n",
    "    string = '-'.join(token_list)\n",
    "    string = tradition2simple(string)    #轉簡體字\n",
    "    jieba_string = list(parser.parse_text(string))\n",
    "    string = str(jieba_string[0]).replace('\\n','')\n",
    "    string = string.replace('(PU -)','')\n",
    "    string = string.split(\" \")\n",
    "    a = str()\n",
    "    for i in string:\n",
    "        if i !=\" \" and i!='':\n",
    "            a += i\n",
    "            a += \" \"\n",
    "    a = a.replace('(PU -)','')\n",
    "    tree = Tree.fromstring(a)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句構解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distance(string_tree,from_leaf,to_leaf):\n",
    "    #計算兩個節點之距離\n",
    "    try:\n",
    "        tree_pos = string_tree.treepositions('leaves')\n",
    "        find_node = ()\n",
    "        for i in tree_pos:\n",
    "            if string_tree[i]==from_leaf:\n",
    "                from_leaf_idx = i\n",
    "            elif string_tree[i]==to_leaf:\n",
    "                to_leaf_idx = i\n",
    "        length = len(from_leaf_idx) if len(from_leaf_idx)<len(to_leaf_idx) else len(to_leaf_idx)\n",
    "        for i in range(length):\n",
    "            if from_leaf_idx[i]==to_leaf_idx[i]:\n",
    "                find_node += (from_leaf_idx[i],)\n",
    "            else:\n",
    "                break\n",
    "        length_from_leaf = len(from_leaf_idx) - len(find_node)\n",
    "        length_to_leaf = len(to_leaf_idx) - len(find_node)\n",
    "        return length_from_leaf+length_to_leaf\n",
    "    except:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx(string_tree,name):\n",
    "    #找尋特定節點之位置\n",
    "    tree_pos = string_tree.treepositions('leaves')\n",
    "    for i in tree_pos:\n",
    "        if string_tree[i]==name:\n",
    "            name_idx = i\n",
    "    return name_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_label(tree, name_1,name_2):\n",
    "    #判斷兩個字在共同節點之下一層節點之label皆為NP或是皆為IP\n",
    "    name1 = find_idx(tree,name_1)\n",
    "    name2 = find_idx(tree,name_2)\n",
    "    find_node_1 = []\n",
    "    find_node_2 = []\n",
    "    for i in range(len(name1)):\n",
    "        if name1[i] == name2[i]:\n",
    "            find_node_1 += [name1[i]]\n",
    "            find_node_2 += [name2[i]]\n",
    "        else:\n",
    "            find_node_1 += [name1[i]]\n",
    "            find_node_2 += [name2[i]]\n",
    "            break\n",
    "    if (tree[find_node_1].label() == 'NP' and tree[find_node_2].label() == 'NP') or (tree[find_node_1].label() == 'IP' and tree[find_node_2].label() == 'IP'):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaf(tree,word):\n",
    "    #判斷特定字是否為樹之子葉\n",
    "    tree_pos = tree.treepositions('leaves')\n",
    "    for i in tree_pos:\n",
    "        if tree[i]==word:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad(tree, name_list, keyword_list):\n",
    "    #當名字多於一人時，判斷哪些名字為類似之名字，i.e. 顧立雄控訴吳志揚以及趨同道在洗錢，則吳志揚以及趨同道為同一類，而顧立雄為另一類\n",
    "    name_bad = {}\n",
    "    distance_list = []\n",
    "    closest_name_list = []\n",
    "    if tree==None:\n",
    "        return None\n",
    "    for i in keyword_list:\n",
    "        if is_leaf(tree,i):\n",
    "            keyword = i\n",
    "    if len(name_list) > 1:\n",
    "        for idx, i in enumerate(name_list):\n",
    "            name_bad[i] = False\n",
    "            distance_list.append(count_distance(tree,i,keyword))\n",
    "#                 if distance_list[idx]=='Error':\n",
    "#                     name_bad[i] = None\n",
    "#             if all(i=='Error' for i in distance_list):\n",
    "#                 return closest_name_list\n",
    "        minidx = np.argmin(distance_list)\n",
    "#             print(\"minidx= \",minidx)\n",
    "        name_bad[name_list[minidx]] = True\n",
    "        for i in range(len(name_list)):\n",
    "            if i != minidx:\n",
    "                dist1 = count_distance(tree,name_list[minidx],keyword)\n",
    "                dist2 = count_distance(tree,name_list[i],keyword)\n",
    "                if dist2=='Error':\n",
    "                    continue\n",
    "                elif dist1 != dist2:\n",
    "                    if is_same_label(tree,name_list[minidx],name_list[i]):\n",
    "                        name_bad[name_list[i]] = True\n",
    "                elif dist1 == dist2: #and is_same_label(tree,name_list[minidx],name_list[i]): #距離一樣且label一樣\n",
    "                    name_bad[name_list[i]] = True\n",
    "        for i in name_bad.items():\n",
    "            if i[1]==True:\n",
    "                closest_name_list.append(i[0])\n",
    "    elif len(name_list)==0:\n",
    "        return []\n",
    "    else:\n",
    "        closest_name_list.append(name_list[0])\n",
    "    return closest_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_active_verb(verb):\n",
    "    #函數判斷是否具有主動的洗錢動詞\n",
    "    r1 = open('../money_laundring_verb.txt', 'r')\n",
    "    verb_list = []\n",
    "    for i in r1:\n",
    "        verb_list.append(tradition2simple(i.strip()))\n",
    "    proved_verb = \"n\"\n",
    "    if verb in verb_list:\n",
    "        proved_verb = \"y\"\n",
    "    r1.close()\n",
    "    return proved_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_passive_verb(verb):\n",
    "    #函數判斷是否具有被動的洗錢動詞\n",
    "    r1 = open('../money_laundring_passive_verb.txt', 'r')\n",
    "    verb_list = []\n",
    "    for i in r1:\n",
    "        verb_list.append(tradition2simple(i.strip()))\n",
    "    proved_verb = \"n\"\n",
    "    if verb in verb_list:\n",
    "        proved_verb = \"y\"\n",
    "    r1.close()\n",
    "    return proved_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_sentence(tree,show_name, closest_criminal_list, keyword_list):\n",
    "    #函數用來判斷句構是否是：名字 +廢字+ 動詞（有洗錢嫌疑的動詞） + 關鍵字/包含關鍵字的子句或廢字\n",
    "    if len(closest_criminal_list) != 0:\n",
    "        closest_criminal = closest_criminal_list[-1]\n",
    "    try:\n",
    "        leaves_upper_node_indexs = []\n",
    "        for i in tree.treepositions('leaves'):#treepositions('leaves')的list裡面是tuple\n",
    "            index = []\n",
    "            for j in range(len(i)-1):\n",
    "                index.append(i[j])\n",
    "            index = tuple(index)\n",
    "            leaves_upper_node_indexs.append(index)\n",
    "        words_and_tags = []\n",
    "        for i in leaves_upper_node_indexs:\n",
    "            words_and_tags.append(str(tree[i]).strip(\"(\").strip(\")\").split(\" \"))\n",
    "        #print(words_and_tags)#每一個項目長這樣： ['NR', '桃園']\n",
    "        #找出離名字最近的動詞(這裡可能之後會有問題，但在句中離名字最近會比用樹的距離來判斷好)\n",
    "\n",
    "        #檢查是否名字在受詞\n",
    "        passive = False\n",
    "        closest_criminal_index = 0\n",
    "        keyword_index = 0\n",
    "        how_close = 10000\n",
    "        for i in words_and_tags:\n",
    "            if i[1] == closest_criminal:\n",
    "                closest_criminal_index = words_and_tags.index(i)\n",
    "                break\n",
    "        for i in words_and_tags:\n",
    "            for j in keyword_list:\n",
    "                if i[1] == j and abs(keyword_index - closest_criminal_index) < how_close :\n",
    "                    keyword_index = words_and_tags.index(i)\n",
    "                    how_close = abs(keyword_index - closest_criminal_index)\n",
    "        if keyword_index < closest_criminal_index:\n",
    "            passive = True\n",
    "\n",
    "\n",
    "        #若名字在敘事句的主詞位置\n",
    "        if passive == False:\n",
    "            name_flag = 0 #假設前面沒有出現過那個名字（一般一個敘事句會提到兩次同個人名字的可能比較小）\n",
    "            bad_sentence = False\n",
    "            v = ''\n",
    "            pass_word = 0\n",
    "            pass_word_list = []\n",
    "            for i in words_and_tags:\n",
    "                if i[1] == closest_criminal:\n",
    "                    name_flag += 1\n",
    "                if name_flag == 1:#目的是判斷現在句子位置在要找的人名之後\n",
    "                    if i[0] == 'VV':\n",
    "                        v = i[1]\n",
    "                        if judge_active_verb(v) == 'y':\n",
    "                            bad_sentence = True\n",
    "                        break\n",
    "                    else:\n",
    "                        pass_word_list.append(i[1])\n",
    "            pass_word = len(pass_word_list)\n",
    "\n",
    "        #若名字在敘事句的受詞位置（往回查找）\n",
    "        if passive == True:\n",
    "            bad_sentence = False\n",
    "            v = ''\n",
    "            pass_word = 0\n",
    "            pass_word_list = []\n",
    "            for i in range(closest_criminal_index):\n",
    "                if words_and_tags[-(len(words_and_tags)-closest_criminal_index)+(-i)][0] == 'VV':\n",
    "                    v = words_and_tags[-(len(words_and_tags)-closest_criminal_index)+(-i)][1]\n",
    "                    if judge_passive_verb(v) == \"y\":\n",
    "                        bad_sentence = True\n",
    "                    break\n",
    "                else:\n",
    "                    pass_word_list.append(words_and_tags[-(len(words_and_tags)-closest_criminal_index)+(-i)][1])\n",
    "            pass_word = len(pass_word_list)\n",
    "        name_and_status = dict()\n",
    "        for j in show_name:\n",
    "                name_and_status[j] = False\n",
    "        for i in closest_criminal_list:\n",
    "            name_and_status[i] = bad_sentence\n",
    "    except:\n",
    "        name_and_status = \"ERROR\"\n",
    "    return name_and_status#[bad_sentence, v, pass_word, pass_word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_s(show_name, name_dict, ans_dict):\n",
    "    wrong_s = []\n",
    "    accuracy=0\n",
    "    for j in range(len(name_dict)):\n",
    "        flag=0\n",
    "        for i in show_name[j]:\n",
    "            try:\n",
    "                if name_dict[j][i]!=ans_dict[j][i]:\n",
    "                    flag=0\n",
    "                    wrong_s.append(j)\n",
    "                    break\n",
    "                flag=1\n",
    "            except:\n",
    "                print(\"i am except\")\n",
    "                continue\n",
    "    \n",
    "        accuracy+=flag    \n",
    "            \n",
    "    return accuracy/float(len(name_dict)), wrong_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_w(show_name, name_dict, ans_dict):\n",
    "    accuracy=0\n",
    "    total_name = 0\n",
    "    for j in range(len(name_dict)):\n",
    "        flag=0\n",
    "        total_name+=len(show_name[j])\n",
    "        for i in show_name[j]:\n",
    "            try:\n",
    "                if name_dict[j][i]==ans_dict[j][i]:\n",
    "                    flag+=1\n",
    "            except:\n",
    "                continue\n",
    "        accuracy+=flag\n",
    "    return accuracy/total_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data():\n",
    "    #整理測試資料\n",
    "    text_data = []\n",
    "    with open('../洗錢測試50句資料csv3.csv','r') as r3:\n",
    "        for i in r3:\n",
    "            text_data.append(i.strip('\"').strip())\n",
    "    text_data.remove(text_data[0])\n",
    "    text_data1 = []\n",
    "    for i in text_data:\n",
    "        text_data1.append(i.split(\",\"))\n",
    "\n",
    "    for i in range(len(text_data1)):\n",
    "        text_data1[i][1] = text_data1[i][1].split(\"，\")\n",
    "        text_data1[i][0] = text_data1[i][0].split(\"，\")\n",
    "    sentence_wanted = [text_data1[i][2] for i in range(len(text_data1))]\n",
    "    show_name = []\n",
    "    for i in range(len(text_data1)):\n",
    "        show_name.append(text_data1[i][1])\n",
    "    ans_dict = []\n",
    "    for i in range(len(text_data1)):\n",
    "        answer_dict = dict()\n",
    "        for j in text_data1[i][0]:\n",
    "            if j in answer_dict:\n",
    "                no_use = 0\n",
    "            else:\n",
    "                answer_dict[j] = True\n",
    "\n",
    "        for k in text_data1[i][1]:\n",
    "            if k in answer_dict:\n",
    "                no_use = 0\n",
    "            else:\n",
    "                answer_dict[k] = False\n",
    "        #del answer_dict[\"無\"] \n",
    "        if \"無\" in answer_dict:\n",
    "            del answer_dict[\"無\"]\n",
    "        ans_dict.append(answer_dict)\n",
    "    name_in_sentence = []\n",
    "    for i in sentence_wanted:\n",
    "        name = ner_demo(i)\n",
    "        name_in_sentence.append(name)\n",
    "        \n",
    "    return sentence_wanted, name_in_sentence, ans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "#     try:\n",
    "    keyword_list = [\"賄賂\",\"貪污\", \"洗錢\", \"詐欺\", \"境外轉帳\", \"詐騙\"]\n",
    "    sentence_wanted, show_name, ans_dict = organize_data()\n",
    "    name_dict = []\n",
    "    show_name_s_list = list()\n",
    "    #加入結巴詞庫\n",
    "    for i in show_name:\n",
    "        for j in i:\n",
    "            write_word(j)\n",
    "\n",
    "    for i in keyword_list:\n",
    "        write_word(i)\n",
    "    keyword_list = list(map(tradition2simple, keyword_list))   #把keyword轉簡體\n",
    "    #讀取結巴\n",
    "    jieba.load_userdict(\"/Users/rikeion/Desktop/Anti_Money_Laundry/user_dict.txt\")\n",
    "    for i,sentence in enumerate(sentence_wanted):\n",
    "        show_name_s = list(map(tradition2simple, show_name[i]))\n",
    "        show_name_s_list.append(show_name_s)\n",
    "        parser = CoreNLPParser('http://localhost:9001')\n",
    "        name = {}\n",
    "        tree = make_tree(sentence,keyword_list)\n",
    "        if tree!=None:\n",
    "            closest_name_list = find_bad(tree, show_name_s, keyword_list)\n",
    "        else:\n",
    "            closest_name_list = None\n",
    "        if closest_name_list == None:\n",
    "            name = {j:False for j in show_name_s}\n",
    "        else:\n",
    "            name = judge_sentence(tree, show_name_s, closest_name_list, keyword_list)\n",
    "        name_dict.append(name)\n",
    "    for k in ans_dict:\n",
    "        for i in k.keys():\n",
    "            k[tradition2simple(i)] = k.pop(i)\n",
    "    for k in ans_dict:\n",
    "        for i in k.keys():\n",
    "            k[tradition2simple(i)] = k.pop(i)\n",
    "    accuracy1,wrong_s = evaluate_accuracy_s(show_name_s_list,name_dict,ans_dict)\n",
    "    print(\"Accuracy by sentence = \",accuracy1)   \n",
    "    accuracy2 = evaluate_accuracy_w(show_name_s_list,name_dict,ans_dict)\n",
    "    print(\"Accuracy by name = \",accuracy2) \n",
    "    print(np.take(sentence_wanted,wrong_s))\n",
    "    return name_dict\n",
    "#     except:\n",
    "#         print(\"Error :(((\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['吳志揚', '鄒德道', '鄒']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林志吉']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇貞昌']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['董瑞斌', '董瑞斌']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['羅文嘉', '蘇貞昌']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林崇成']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['孟晚舟']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吳宣', '孫武生', '孫男拿', '孫武生', '顏伯萊', '孫男才', '吳宣']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['孟晚舟', '孟晚舟']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "['李新', '郭新政', '李新」', '羅淑蕾', '郭新政', '李新', '羅', '郭與盛', '郭新政', '羅淑蕾']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['洪俊凱']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['巫蕙玲', '林崇成', '陳宏洲', '紀炳場', '郜振傑', '李青芬']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['孟晚舟', 'T-Mobile']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['李新', '郭新政', '羅淑蕾', '李新']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['何志平']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林琪容', '林佳穎']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陳慶男', '陳盧昭霞', '陳偉志', '簡良鑑', '李維峰']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['施永華', '陳玉珍']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['鄭美玲', '董瑞斌', '謝同仁']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林峰', '邵永華']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['邱淑貞']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡慧中', '何志平', '何志平']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陳誌偉']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陳志鵬', '武元鐘', '林國豪']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陳家欽']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇建榮', '鄭貞茂']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['李新生', '郭新政']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['何志平']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['楊金龍']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陳誌偉']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陳沅偉', '陳柏權', '陳男']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['葉美麗']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蔡嘉偉']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['張兆順']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['簡良鑑']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['羅淑蕾', '郭新政', '李新影', '蕭夫', '郭新政']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['胡忠信', '山根敬']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['鄭美玲', '廖燦昌', '蔡麗雪']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['曾銘宗']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇建榮']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['王立群']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇建榮', '蘇建榮']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['邱淑貞']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['董耀濱']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林琪容', '林佳穎', '林佳穎', '胡珮如', '林琪容']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['蔡嘉偉', '蔡嫌', '瑪莎拉蒂']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['余依珊', '孫凱倫', '孫男']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['KPMG', '朱成光']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['朱成光']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡慧中', '何志平', '何志平']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['胡慧中', '何志平', 'Energy']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡則華']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡則華']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['高豐霖']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林郁平', '高姓男子']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林佳穎', '林佳穎']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "['林佳穎', '林琪容', '林佳穎', '胡珮如', '林', '胡']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡忠信', '南懷瑾', '伍必霈', '伍必霈']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['胡忠信', '山根敬', '伍必霈', '胡忠信']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡忠信']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許榮棋', '葉美麗', '司茂豐']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['許榮', '葉美麗', '葉美麗', '茂豐']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['葉美麗']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['葉美麗']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許榮棋', '葉美麗']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許榮棋', '葉美麗']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇建榮']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇貞昌', '顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['賈達恩', 'MohammedAlJadaan']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['宋國業', '陳傳宗', '許永欽', '黃錦秋']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黃錦秋', '許永欽', '趙建銘']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['林邦樑', '余麗貞', '張曉雯', '黃謀信']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許永欽']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇貞昌', '蘇揆']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇貞昌']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蘇貞昌', '顧立雄', '蘇建榮', '徐國勇', '雷仲達']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['蘇貞昌', '顧立雄', '釋利多']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蔡英文']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['孫鐵志']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['南懷瑾', '胡忠信', '蔡玉真', '胡忠信', '胡忠信', '蔡玉真', '黃琴雅']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['胡忠信']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黃立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黃立雄', '黃立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黃立雄', '黃芳彥', '黃芳彥']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['侯建業', '吳自心', '張兆順', '丁新典']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['張兆順']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['張兆順']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['余麗貞', '賴清德', '施俊吉', '羅秉成']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['余麗貞']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['莊琇媛']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['莊琇媛']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吳當傑']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['董瑞斌']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['賴清德']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['蔡清祥']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['張兆順']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['趙雙傑', '賴清德']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['賴清德']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['顧立雄']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['賴清德']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['余麗貞']\n",
      "INFO:tensorflow:Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from ./data_path_save/1559725764/checkpoints/model-633400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['賴英門']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting dropped connection: localhost\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 74418\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 61632\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 209483\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 23740\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 134991\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 77380\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 106508\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 86686\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 208189\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 94029\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 58360\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 118191\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 26658\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 67167\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 57353\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 86891\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 54094\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 75287\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 94814\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 38134\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 44616\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 46081\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 29205\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 80691\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 33047\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 17737\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 110264\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 89253\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 92624\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 59174\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 43478\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 131042\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 90199\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 87869\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 133828\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 59874\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 118900\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 120282\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 109208\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 128478\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 46998\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 139085\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 96724\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 129776\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 13037\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 118346\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 70504\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 54897\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 34541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 35953\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 75277\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 70948\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 53392\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 21605\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 40941\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 50805\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 80759\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 117117\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 99835\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 19067\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 161466\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 61837\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 8523\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 70514\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 30330\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 99982\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 87197\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 17786\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 91062\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 127895\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 100846\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 129193\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 52118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "i am except\n",
      "Accuracy by sentence =  0.6422764227642277\n",
      "Accuracy by name =  0.6530612244897959\n",
      "[\"桃園地檢署偵辦前桃園縣長吳志揚隨行秘書鄒德道等人涉嫌收受楊梅地區的地主100萬元現金及價值200萬元的BMW案，並居中介紹官員協助土地變更案，今偵查終結，依貪污治罪條例起訴將鄒等人起訴'\"\n",
      " '\\\\xa0檢方認為，吳宣在2018年6月間加入孫武生販毒集團，協助運送裝有毒品的菸盒到買家指定地點，並替孫男拿取買家投放孫男刺青工作室信箱裝有金錢的信封，涉嫌洗錢，還替孫武生、班特到萬華區刀品店購買殺害顏伯萊的兩把開山刀、鏈鋸等，案發時更在命案現場永和河濱公園對岸把風，後因看不清楚，孫男才讓吳宣先行離開'\n",
      " '美國司法部在台灣時間今晨4時許高調舉行記者會，公佈起訴華為、華為子公司及華為副董事長孟晚舟共23項罪名，包括銀行詐欺、密謀洗錢、竊取美國公司機密等，並宣布會向加拿大要求引渡孟晚舟'\n",
      " '\\\\xa0已故台北市議員李新前女友郭新政成立「誰摔死了李新」臉書專頁，至今已推出9支影片，並找來資深主播盛竹如拍片，影射前立委羅淑蕾涉入郭新政的珠寶詐騙案害死李新，羅對郭與盛提告，但是事情並沒有告一段落，郭新政指控羅淑蕾找人電話恐嚇盛竹如，劇情堪比連續劇'\n",
      " '由洪俊凱在網路搜尋泰國電話號碼，每次設定500至1000組號碼後，以群呼系統傳送語音訊息給泰國民眾，佯稱電話號碼將被封鎖，當民眾詢問時，即轉接至詐騙成員手機，誆騙對方涉及洗錢、販毒，銀行存款要匯出列管等'\n",
      " '檢方偵辦中山分局警員涉嫌集體收賄包庇酒店，查出日式酒店「曉」及其前身「立邦酒店」每月提供4萬元現金給轄區警方，換取包庇、通風報信，13年間共收賄780萬元，北檢去年7月依違反《貪污治罪條例》的行賄罪、妨害風化罪起訴酒店業者巫蕙玲等3人，並以收賄罪起訴中山一派出所所長林崇成、警員陳宏洲、紀炳場、郜振傑等10警員，其中3警認罪，但有7警堅不認罪，因此請求法院重判7警，另林崇成被查出將賄款存入妻子李青芬帳戶，檢方另依違反《洗錢防治條例》一併起訴'\n",
      " '而在去年台北市議員李新跳樓自殺周年忌日，郭新政到台北地檢署，指控羅淑蕾與珠寶商嚴嘉慧偽造當票，害她及李新被外界抹黑指責是詐欺犯、洗錢犯'\n",
      " '何志平前天身穿深色囚衣出庭聆聽判決，比昔日消瘦不少，法官指出，貪污是嚴重罪行，會破壞法治、影響民生，但稱讚何的為人處事，因羈押期間曾幫助其他嫌犯，加上在香港擁有的身分地位、再犯機率低，即使僅判刑數月「已有足夠阻嚇力」，因此判刑3年'\n",
      " '2006年6月底，施永華投資股市慘賠1億元，他認為遭投顧公司詐騙，拜託陳玉珍幫忙，陳女卻藉機向他索賄近3000萬元，施男認為陳女貪得無厭，憤而登報紙廣告揭發陳的劣行，並向當時的特偵組提告'\n",
      " '此外，報導引述消息人士稱，林峰名義上是人民公僕，私下與涉案人士交往密切，貪污受賄，用國家警力為邵永華等人打開「方便之門」'\n",
      " '台灣知名女星胡慧中演出《歡顏》、《霸王花》聞名，1998年嫁給香港民政事務局前局長、眼科醫生何志平，何志平因涉嫌為中資企業行賄查德與烏干達高官，前年在美國紐約被捕，遭裁定貪污、洗錢等7項罪名成立，最高恐被判入獄65年'\n",
      " '台中男子陳志鵬、武元鐘、林國豪及綽號「小玉」等4人，涉嫌集資100萬元在越南設立垮境詐騙機房專騙泰國人'\n",
      " '台北市議員李新生前女友郭新政捲入珠寶詐騙案，被法院依偽造文書罪判刑4月定讞' '台中檢警掌握男子陳誌偉在台灣擔任詐騙集團的總務'\n",
      " '檢方指出，男子陳沅偉和陳柏權分別擔任詐騙集團的現場負責人和電腦手，去年3月，陳男等人參與綽號「鬼哥」在印尼籌組的詐騙機房，以「假檢警真詐財」方式向中國的民眾行騙，2個月時間就詐得989萬餘元，詐團成員依工作表現返台後再領取薪資、獎金'\n",
      " '檢警4月底循線在台中，將幕後金主蔡嘉偉(39歲、在押)拘提到案，查扣現金552萬4，852元、高級房車26輛鑰匙、不動產(房屋1幢、土地3筆)，並扣押境內帳戶21個、境外洗錢帳戶2個，金額達5，923萬餘元'\n",
      " '2017年11月10日，高雄地檢署傳喚慶富前執行長簡良鑑，訊後依涉犯銀行法特別詐欺等罪嫌，且有串證及逃亡之虞，向法院聲押，獲裁處以300萬元交保'\n",
      " '羅淑蕾表示，除了珠寶案之外，郭新政還涉入洗錢案關說案，誰摔死李新影片中所提到的蕭夫人，其實就是郭新政本人'\n",
      " '台中婦人林琪容前年2月從簽賭六合彩的賭客晉升當組頭，竟與任職大里農會的外甥女林佳穎共同經營簽賭站，利用林女職務之便開戶轉存放賭金，至去年7月共收帳8075萬元，而為避免被查緝，林佳穎還找來同事胡珮如窺探警方、國稅局寄發公函，台中地檢署今依賭博、洗錢、洩密等罪將林琪容等7名上、下游組頭一併起訴'\n",
      " '2年前因領軍「剝皮妹」集團被逮入獄的花式撞球好手余依珊（37歲），不料她老公孫凱倫（38歲）竟號召8名同夥在中山區「重啟剝皮爐灶」，甚至組跨境詐騙集團，吸收多名中國籍女子詐騙「多金歐吉桑」，孫男從中獲利頗豐，坐擁3百萬保時捷、2百萬勞力士名錶、伯爵錶'\n",
      " '台灣知名女星胡慧中於1988年嫁給香港民政事務局前局長何志平，甘心淡出演藝圈做家庭主婦，然老公前年11月在紐約被捕，罪名是違反美國海外反腐敗行為法、洗錢及共謀，至今美國聯邦法庭判決結果出爐，照何志平罪證，最高可判65年，然他最終以3年刑期及繳罰款千萬作結'\n",
      " '胡慧中老公、前香港民政事務局局長何志平，被捕時擔任香港中華能源基金會秘書長，該組織由總部在上海的大陸華信能源公司(CEFC Chi無 Energy)資助，而據《蘋果》報導，他涉嫌替華信能源公司換取合約，賄賂查德和烏干達高官，去年底在美國遭到裁定5項貪汙、2項洗錢案成立，最重刑期高達65年'\n",
      " '28歲男子高豐霖，在北市內湖科技園區以經營貿易公司為幌子，實際上從事地下匯兌洗錢，成員外出收款時還會攜槍防身'\n",
      " '台中：大里區農會職員林佳穎，與阿姨林琪容共營六合彩簽賭站，短短1年半大賺8000多萬元，因擔心違法情事被發現，林佳穎還利用職務之便、先後洗錢達8075萬元，並找同事窺探警局情資，檢方依賭博、洗錢等罪起訴林等9人'\n",
      " '台中市大里區農會職員林佳穎，從2017年2月起與阿姨林琪容共同經營六合彩簽賭站，2人在短短1年半時間就大賺8千多萬元，因擔心違法情事被發現，林佳穎還利用農會職務之便、先後洗錢達8075萬元，並找同事胡珮如窺探警局情資，台中檢方依賭博、洩密、洗錢防治法等罪起訴林、胡等共9人'\n",
      " '台北地檢署偵辦兆豐洗錢案，以查無不法為由簽結後，媒體人胡忠信指控已故國學大師南懷瑾之子南國熙，疑是為日本山口組洗錢的關鍵人，另與南熟識的保勝光學董事長伍必霈也捲入，告發南、伍2人涉嫌違反洗錢防制法，台北地檢署經1年多查證後，認為罪證不足，今將南國熙、伍必霈2人不起訴'\n",
      " '北檢簽結兆豐案後，胡忠信懷疑南國熙和山口組洗錢高手山根敬，在香港合組空殼公司涉及洗錢，具狀向北檢告發南國熙、保勝光學董事長伍必霈2人涉嫌洗錢，胡忠信並多次在媒體上指控此事'\n",
      " '不過4日台灣之聲負責人許榮棋具名刊登陳情廣告，要求台北地檢署偵辦葉美麗透過子公司茂豐涉及洗錢一事'\n",
      " '許榮棋針對葉美麗涉及洗錢，提出三項陳情，一、葉美麗是茂豐董事長，去年9月13日私自將公款286.36萬美元（約新台幣8826萬元）匯到香港「BAO BUI INTER無TION  LIMITED」帳戶'\n",
      " '三、葉美麗涉掏空約8千萬元公款，到女兒帳戶洗錢，涉洗錢防制法洗錢罪，及刑法特別背信、證交法背信罪等，陳情提告'\n",
      " '許榮棋也具名刊登陳情廣告要求北檢偵辦葉美麗涉及洗錢一事'\n",
      " '台灣之聲負責人許榮棋也具名刊登陳情廣告，要求台北地檢署偵辦葉美麗涉及洗錢一事，並且要求葉美麗回應種種疑點'\n",
      " '其中，新任的警政署政風室主任黃錦秋是司法官35期，為第一位女性警政署政風室主任，另行政院洗錢辦執祕許永欽，為司法官34期，偵辦過多起重大經濟犯罪案件，辦案經驗豐富，曾是北檢首位派駐金管會的檢察官，偵辦過扁婿趙建銘涉台開內線交易案、勁永禿鷹案，是高檢署「反股市禿鷹小組」成員之一'\n",
      " '（圖／中時資料照）涉嫌詐欺並已捲款出逃的貴婦奈奈，真的跑到美國舊金山？有人出面爆料指貴婦奈奈的公公黃立雄是某佛團的大金主，佛團涉嫌大量洗錢到加拿大的愛德華王子島，所以合理懷疑貴婦奈奈全家藏身愛德華王子島'\n",
      " '根據《三立新聞》報導，某佛團的前團員爆料說，黃立雄是佛團女上師的鐵粉，也是大金主，大家都認識，佛團女接班人涉洗錢到加拿大P島（愛德華王子島），把P島當總部所在地，並鼓勵信徒以投資移民身份，移民至P島，因此，黃立雄全家極可能擁有加拿大護照，捲款後，出逃至加拿大，而非美國'\n",
      " '不過，傳聞也有一說，就是黃立雄與黃芳彥有好交情，每次到舊金山兩人就會約喝下午茶，黃芳彥因捲入SOGO禮券案、為扁家洗錢等弊案而祕密躲到美國，據傳就住在加州爾灣，貴婦奈奈一家躲到這裡，也不無可能']\n"
     ]
    }
   ],
   "source": [
    "#計入NER後的成果:65%\n",
    "name_dict = demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting dropped connection: localhost\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 61632\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 209483\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 23740\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 133644\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 77380\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 109502\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 86686\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 207044\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 94029\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 58360\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 118191\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 26658\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 67167\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 57353\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 86891\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 54094\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 75287\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 94814\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 38134\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 44616\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 46081\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 31038\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 80691\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 33047\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 17737\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 111731\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 89253\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 92624\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 59174\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 45121\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 131042\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 90199\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 87869\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 133828\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 59874\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 118900\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 120282\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 109208\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 128478\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 46998\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 139826\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 96724\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 129776\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 13037\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 118429\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 70504\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 54897\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 34541\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 35953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 75277\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 70948\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 53392\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 21605\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 40941\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 50805\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 80759\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 117117\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 99835\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 19067\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 16807\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 161466\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 61837\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 8523\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 70514\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 30330\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 99982\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 87197\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 17786\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 91062\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 127895\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 100846\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 129193\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 99801\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 179781\n",
      "http://localhost:9001 \"POST /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cparse%22%7D HTTP/1.1\" 200 52118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by sentence =  0.6666666666666666\n",
      "Accuracy by name =  0.7208121827411168\n",
      "[\"桃園地檢署偵辦前桃園縣長吳志揚隨行秘書鄒德道等人涉嫌收受楊梅地區的地主100萬元現金及價值200萬元的BMW案，並居中介紹官員協助土地變更案，今偵查終結，依貪污治罪條例起訴將鄒等人起訴'\"\n",
      " '\\\\xa0檢方認為，吳宣在2018年6月間加入孫武生販毒集團，協助運送裝有毒品的菸盒到買家指定地點，並替孫男拿取買家投放孫男刺青工作室信箱裝有金錢的信封，涉嫌洗錢，還替孫武生、班特到萬華區刀品店購買殺害顏伯萊的兩把開山刀、鏈鋸等，案發時更在命案現場永和河濱公園對岸把風，後因看不清楚，孫男才讓吳宣先行離開'\n",
      " '美國司法部在台灣時間今晨4時許高調舉行記者會，公佈起訴華為、華為子公司及華為副董事長孟晚舟共23項罪名，包括銀行詐欺、密謀洗錢、竊取美國公司機密等，並宣布會向加拿大要求引渡孟晚舟'\n",
      " '\\\\xa0已故台北市議員李新前女友郭新政成立「誰摔死了李新」臉書專頁，至今已推出9支影片，並找來資深主播盛竹如拍片，影射前立委羅淑蕾涉入郭新政的珠寶詐騙案害死李新，羅對郭與盛提告，但是事情並沒有告一段落，郭新政指控羅淑蕾找人電話恐嚇盛竹如，劇情堪比連續劇'\n",
      " '由洪俊凱在網路搜尋泰國電話號碼，每次設定500至1000組號碼後，以群呼系統傳送語音訊息給泰國民眾，佯稱電話號碼將被封鎖，當民眾詢問時，即轉接至詐騙成員手機，誆騙對方涉及洗錢、販毒，銀行存款要匯出列管等'\n",
      " '檢方偵辦中山分局警員涉嫌集體收賄包庇酒店，查出日式酒店「曉」及其前身「立邦酒店」每月提供4萬元現金給轄區警方，換取包庇、通風報信，13年間共收賄780萬元，北檢去年7月依違反《貪污治罪條例》的行賄罪、妨害風化罪起訴酒店業者巫蕙玲等3人，並以收賄罪起訴中山一派出所所長林崇成、警員陳宏洲、紀炳場、郜振傑等10警員，其中3警認罪，但有7警堅不認罪，因此請求法院重判7警，另林崇成被查出將賄款存入妻子李青芬帳戶，檢方另依違反《洗錢防治條例》一併起訴'\n",
      " '而在去年台北市議員李新跳樓自殺周年忌日，郭新政到台北地檢署，指控羅淑蕾與珠寶商嚴嘉慧偽造當票，害她及李新被外界抹黑指責是詐欺犯、洗錢犯'\n",
      " '何志平前天身穿深色囚衣出庭聆聽判決，比昔日消瘦不少，法官指出，貪污是嚴重罪行，會破壞法治、影響民生，但稱讚何的為人處事，因羈押期間曾幫助其他嫌犯，加上在香港擁有的身分地位、再犯機率低，即使僅判刑數月「已有足夠阻嚇力」，因此判刑3年'\n",
      " '2006年6月底，施永華投資股市慘賠1億元，他認為遭投顧公司詐騙，拜託陳玉珍幫忙，陳女卻藉機向他索賄近3000萬元，施男認為陳女貪得無厭，憤而登報紙廣告揭發陳的劣行，並向當時的特偵組提告'\n",
      " '此外，報導引述消息人士稱，林峰名義上是人民公僕，私下與涉案人士交往密切，貪污受賄，用國家警力為邵永華等人打開「方便之門」'\n",
      " '台灣知名女星胡慧中演出《歡顏》、《霸王花》聞名，1998年嫁給香港民政事務局前局長、眼科醫生何志平，何志平因涉嫌為中資企業行賄查德與烏干達高官，前年在美國紐約被捕，遭裁定貪污、洗錢等7項罪名成立，最高恐被判入獄65年'\n",
      " '台中男子陳志鵬、武元鐘、林國豪及綽號「小玉」等4人，涉嫌集資100萬元在越南設立垮境詐騙機房專騙泰國人'\n",
      " '台北市議員李新生前女友郭新政捲入珠寶詐騙案，被法院依偽造文書罪判刑4月定讞' '台中檢警掌握男子陳誌偉在台灣擔任詐騙集團的總務'\n",
      " '檢方指出，男子陳沅偉和陳柏權分別擔任詐騙集團的現場負責人和電腦手，去年3月，陳男等人參與綽號「鬼哥」在印尼籌組的詐騙機房，以「假檢警真詐財」方式向中國的民眾行騙，2個月時間就詐得989萬餘元，詐團成員依工作表現返台後再領取薪資、獎金'\n",
      " '檢警4月底循線在台中，將幕後金主蔡嘉偉(39歲、在押)拘提到案，查扣現金552萬4，852元、高級房車26輛鑰匙、不動產(房屋1幢、土地3筆)，並扣押境內帳戶21個、境外洗錢帳戶2個，金額達5，923萬餘元'\n",
      " '2017年11月10日，高雄地檢署傳喚慶富前執行長簡良鑑，訊後依涉犯銀行法特別詐欺等罪嫌，且有串證及逃亡之虞，向法院聲押，獲裁處以300萬元交保'\n",
      " '羅淑蕾表示，除了珠寶案之外，郭新政還涉入洗錢案關說案，誰摔死李新影片中所提到的蕭夫人，其實就是郭新政本人'\n",
      " '胡忠信提出告發後受訪表示，山根敬和南國熙在香港合組一個空頭公司在洗錢，刑事局也有得到此案的法院判決，還有南國熙涉及台灣境內、境外的洗錢，於是他「基於一個公民的職責」，向4個單位控訴，他還說，因為發覺到獲得的訊息「非常非常嚴重」，牽涉到山口組洗錢的車手「山根敬」'\n",
      " '台中婦人林琪容前年2月從簽賭六合彩的賭客晉升當組頭，竟與任職大里農會的外甥女林佳穎共同經營簽賭站，利用林女職務之便開戶轉存放賭金，至去年7月共收帳8075萬元，而為避免被查緝，林佳穎還找來同事胡珮如窺探警方、國稅局寄發公函，台中地檢署今依賭博、洗錢、洩密等罪將林琪容等7名上、下游組頭一併起訴'\n",
      " '2年前因領軍「剝皮妹」集團被逮入獄的花式撞球好手余依珊（37歲），不料她老公孫凱倫（38歲）竟號召8名同夥在中山區「重啟剝皮爐灶」，甚至組跨境詐騙集團，吸收多名中國籍女子詐騙「多金歐吉桑」，孫男從中獲利頗豐，坐擁3百萬保時捷、2百萬勞力士名錶、伯爵錶'\n",
      " '台灣知名女星胡慧中於1988年嫁給香港民政事務局前局長何志平，甘心淡出演藝圈做家庭主婦，然老公前年11月在紐約被捕，罪名是違反美國海外反腐敗行為法、洗錢及共謀，至今美國聯邦法庭判決結果出爐，照何志平罪證，最高可判65年，然他最終以3年刑期及繳罰款千萬作結'\n",
      " '胡慧中老公、前香港民政事務局局長何志平，被捕時擔任香港中華能源基金會秘書長，該組織由總部在上海的大陸華信能源公司(CEFC Chi無 Energy)資助，而據《蘋果》報導，他涉嫌替華信能源公司換取合約，賄賂查德和烏干達高官，去年底在美國遭到裁定5項貪汙、2項洗錢案成立，最重刑期高達65年'\n",
      " '28歲男子高豐霖，在北市內湖科技園區以經營貿易公司為幌子，實際上從事地下匯兌洗錢，成員外出收款時還會攜槍防身'\n",
      " '台中：大里區農會職員林佳穎，與阿姨林琪容共營六合彩簽賭站，短短1年半大賺8000多萬元，因擔心違法情事被發現，林佳穎還利用職務之便、先後洗錢達8075萬元，並找同事窺探警局情資，檢方依賭博、洗錢等罪起訴林等9人'\n",
      " '台中市大里區農會職員林佳穎，從2017年2月起與阿姨林琪容共同經營六合彩簽賭站，2人在短短1年半時間就大賺8千多萬元，因擔心違法情事被發現，林佳穎還利用農會職務之便、先後洗錢達8075萬元，並找同事胡珮如窺探警局情資，台中檢方依賭博、洩密、洗錢防治法等罪起訴林、胡等共9人'\n",
      " '南懷瑾之子南國熙疑涉洗錢案，檢不起訴'\n",
      " '台北地檢署偵辦兆豐洗錢案，以查無不法為由簽結後，媒體人胡忠信指控已故國學大師南懷瑾之子南國熙，疑是為日本山口組洗錢的關鍵人，另與南熟識的保勝光學董事長伍必霈也捲入，告發南、伍2人涉嫌違反洗錢防制法，台北地檢署經1年多查證後，認為罪證不足，今將南國熙、伍必霈2人不起訴'\n",
      " '北檢簽結兆豐案後，胡忠信懷疑南國熙和山口組洗錢高手山根敬，在香港合組空殼公司涉及洗錢，具狀向北檢告發南國熙、保勝光學董事長伍必霈2人涉嫌洗錢，胡忠信並多次在媒體上指控此事'\n",
      " '南國熙因遭胡忠信告發涉山口組洗錢案，台北地檢署、刑事局曾於2017年12月6日、2018年1月10日、11日，分別傳喚、約詢南國熙'\n",
      " '至於胡忠信告發南國熙、伍必霈涉山口組洗錢案，檢方認為罪證不足，今將南國熙2人不起訴'\n",
      " '不過4日台灣之聲負責人許榮棋具名刊登陳情廣告，要求台北地檢署偵辦葉美麗透過子公司茂豐涉及洗錢一事'\n",
      " '許榮棋針對葉美麗涉及洗錢，提出三項陳情，一、葉美麗是茂豐董事長，去年9月13日私自將公款286.36萬美元（約新台幣8826萬元）匯到香港「BAO BUI INTER無TION  LIMITED」帳戶'\n",
      " '三、葉美麗涉掏空約8千萬元公款，到女兒帳戶洗錢，涉洗錢防制法洗錢罪，及刑法特別背信、證交法背信罪等，陳情提告'\n",
      " '許榮棋也具名刊登陳情廣告要求北檢偵辦葉美麗涉及洗錢一事'\n",
      " '台灣之聲負責人許榮棋也具名刊登陳情廣告，要求台北地檢署偵辦葉美麗涉及洗錢一事，並且要求葉美麗回應種種疑點'\n",
      " '其中，新任的警政署政風室主任黃錦秋是司法官35期，為第一位女性警政署政風室主任，另行政院洗錢辦執祕許永欽，為司法官34期，偵辦過多起重大經濟犯罪案件，辦案經驗豐富，曾是北檢首位派駐金管會的檢察官，偵辦過扁婿趙建銘涉台開內線交易案、勁永禿鷹案，是高檢署「反股市禿鷹小組」成員之一'\n",
      " '已故國學大師南懷瑾之子南國熙，不滿新新聞雜誌、名嘴胡忠信、蔡玉真等人自去年起一再指他涉及日本山口組洗錢案，控告胡忠信等人妨害名譽，台北地檢署認為罪證不足，今早將胡忠信、蔡玉真及新新聞記者黃琴雅3不起訴'\n",
      " '（圖／中時資料照）涉嫌詐欺並已捲款出逃的貴婦奈奈，真的跑到美國舊金山？有人出面爆料指貴婦奈奈的公公黃立雄是某佛團的大金主，佛團涉嫌大量洗錢到加拿大的愛德華王子島，所以合理懷疑貴婦奈奈全家藏身愛德華王子島'\n",
      " '根據《三立新聞》報導，某佛團的前團員爆料說，黃立雄是佛團女上師的鐵粉，也是大金主，大家都認識，佛團女接班人涉洗錢到加拿大P島（愛德華王子島），把P島當總部所在地，並鼓勵信徒以投資移民身份，移民至P島，因此，黃立雄全家極可能擁有加拿大護照，捲款後，出逃至加拿大，而非美國'\n",
      " '不過，傳聞也有一說，就是黃立雄與黃芳彥有好交情，每次到舊金山兩人就會約喝下午茶，黃芳彥因捲入SOGO禮券案、為扁家洗錢等弊案而祕密躲到美國，據傳就住在加州爾灣，貴婦奈奈一家躲到這裡，也不無可能']\n"
     ]
    }
   ],
   "source": [
    "#不計入NER正確率之前的成果：72%\n",
    "name_dict1 = demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
